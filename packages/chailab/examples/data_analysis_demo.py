"""
ChaiLab Data Analysis & Visualization Demo
=========================================

This demo showcases Python's data analysis and visualization capabilities
integrated with ChaiLab's shadcn/ui interface. It demonstrates how to
create interactive data analysis tools with beautiful web interfaces.

Features:
â€¢ Data generation and manipulation
â€¢ Statistical analysis
â€¢ Data visualization simulation
â€¢ Machine learning model simulation
â€¢ Real-time data processing
â€¢ Interactive parameter tuning
"""

import chailab as cl
from chailab.ui import Input, Button, Card, CardHeader, CardTitle, CardContent
import json
import random
import math
import statistics
import time
from datetime import datetime
import re

def data_analysis_demo(analysis_type, data_input, parameters):
    """Comprehensive data analysis demonstration"""

    # Handle slider inputs (convert from list to single value)
    if parameters is None:
        parameters = ""
    elif isinstance(parameters, (list, tuple)):
        parameters = parameters[0] if parameters else ""

    results = []
    results.append("ðŸ“Š **ChaiLab Data Analysis Demo**")
    results.append("=" * 45)

    # Parse data input
    data = []
    if data_input:
        # Try to parse as numbers
        numbers = re.findall(r'[-+]?\d*\.?\d+', data_input)
        if numbers:
            data = [float(x) for x in numbers]
        else:
            # Use as text data
            data = data_input.split()
    else:
        # Generate sample data based on analysis type
        if analysis_type == "normal":
            data = [random.gauss(100, 15) for _ in range(100)]
        elif analysis_type == "uniform":
            data = [random.uniform(0, 100) for _ in range(100)]
        elif analysis_type == "exponential":
            data = [random.expovariate(0.1) for _ in range(100)]
        else:
            data = [random.gauss(50, 10) for _ in range(50)]

    results.append(f"**Analysis Type:** {analysis_type}")
    results.append(f"**Dataset Size:** {len(data)} points")
    results.append("")

    if analysis_type in ["basic", "all"]:
        results.append("ðŸ“ˆ **Basic Statistics**")
        results.append("-" * 25)

        if data:
            results.append(f"â€¢ Mean: {statistics.mean(data):.3f}")
            results.append(f"â€¢ Median: {statistics.median(data):.3f}")
            results.append(f"â€¢ Mode: {statistics.mode(data) if len(set(data)) < len(data) else 'No unique mode':.3f}")
            results.append(f"â€¢ Standard Deviation: {statistics.stdev(data):.3f}")
            results.append(f"â€¢ Variance: {statistics.variance(data):.3f}")
            results.append(f"â€¢ Range: {max(data) - min(data):.3f}")
            results.append(f"â€¢ Min: {min(data):.3f}")
            results.append(f"â€¢ Max: {max(data):.3f}")

            # Quartiles
            sorted_data = sorted(data)
            q1 = statistics.quantiles(sorted_data, n=4)[0]
            q3 = statistics.quantiles(sorted_data, n=4)[2]
            iqr = q3 - q1
            results.append(f"â€¢ Q1 (25th percentile): {q1:.3f}")
            results.append(f"â€¢ Q3 (75th percentile): {q3:.3f}")
            results.append(f"â€¢ IQR: {iqr:.3f}")
        else:
            results.append("â€¢ No numeric data available")

    if analysis_type in ["distribution", "all"]:
        results.append("ðŸ“Š **Distribution Analysis**")
        results.append("-" * 30)

        if data:
            # Frequency distribution
            if len(data) > 10:
                # Create bins
                min_val, max_val = min(data), max(data)
                bin_width = (max_val - min_val) / 5
                bins = [min_val + i * bin_width for i in range(6)]

                distribution = {}
                for i in range(5):
                    bin_start = bins[i]
                    bin_end = bins[i + 1]
                    count = sum(1 for x in data if bin_start <= x < bin_end)
                    distribution[f"{bin_start:.1f}-{bin_end:.1f}"] = count

                results.append("â€¢ Frequency Distribution:")
                for bin_range, count in distribution.items():
                    results.append(f"  {bin_range}: {count} items")
            else:
                results.append("â€¢ Sample values:")
                for i, val in enumerate(data[:10]):
                    results.append(f"  [{i+1}] {val:.3f}")

            # Outliers detection
            if len(data) > 4:
                q1, q3 = statistics.quantiles(data, n=4)[0], statistics.quantiles(data, n=4)[2]
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr

                outliers = [x for x in data if x < lower_bound or x > upper_bound]
                results.append(f"â€¢ Potential outliers: {len(outliers)}")
                if outliers:
                    results.append(f"  Outliers: {['.2f' for x in outliers[:5]]}")
        else:
            results.append("â€¢ No data for distribution analysis")

    if analysis_type in ["correlation", "all"]:
        results.append("ðŸ”— **Correlation Analysis**")
        results.append("-" * 30)

        if len(data) > 10:
            # Generate correlated data
            base_data = data
            noise1 = [random.gauss(0, 5) for _ in range(len(data))]
            noise2 = [random.gauss(0, 3) for _ in range(len(data))]

            correlated_data1 = [x + n for x, n in zip(base_data, noise1)]
            correlated_data2 = [x * 0.8 + n for x, n in zip(base_data, noise2)]

            # Simple correlation coefficient
            def correlation(x, y):
                n = len(x)
                sum_x = sum(x)
                sum_y = sum(y)
                sum_xy = sum(a * b for a, b in zip(x, y))
                sum_x2 = sum(a * a for a in x)
                sum_y2 = sum(b * b for b in y)

                numerator = n * sum_xy - sum_x * sum_y
                denominator = math.sqrt((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2))

                return numerator / denominator if denominator != 0 else 0

            corr_coef = correlation(base_data, correlated_data1)
            results.append(f"â€¢ Correlation coefficient: {corr_coef:.3f}")

            if abs(corr_coef) > 0.7:
                results.append("â€¢ Strong correlation detected!")
            elif abs(corr_coef) > 0.3:
                results.append("â€¢ Moderate correlation detected")
            else:
                results.append("â€¢ Weak correlation")

            results.append(f"â€¢ Correlation strength: {'Strong' if abs(corr_coef) > 0.7 else 'Moderate' if abs(corr_coef) > 0.3 else 'Weak'}")
        else:
            results.append("â€¢ Need more data points for correlation analysis")

    if analysis_type in ["machine_learning", "all"]:
        results.append("ðŸ¤– **Machine Learning Simulation**")
        results.append("-" * 35)

        if data:
            # Simulate ML training
            results.append("â€¢ Training simulated model...")

            # Feature engineering simulation
            features = []
            for val in data:
                features.append({
                    'original': val,
                    'squared': val ** 2,
                    'sqrt': math.sqrt(abs(val)),
                    'log': math.log(abs(val) + 1)
                })

            results.append(f"â€¢ Feature engineering: {len(features)} samples, {len(features[0])} features")

            # Model training simulation
            train_size = int(len(data) * 0.7)
            train_data = data[:train_size]
            test_data = data[train_size:]

            results.append(f"â€¢ Training set: {len(train_data)} samples")
            results.append(f"â€¢ Test set: {len(test_data)} samples")

            # Simulate model performance
            train_mean = statistics.mean(train_data)
            test_mean = statistics.mean(test_data)

            # Simple prediction accuracy simulation
            accuracy = max(0.1, 1 - abs(train_mean - test_mean) / max(abs(train_mean), abs(test_mean)))
            results.append(f"â€¢ Model accuracy: {accuracy:.1%}")

            if accuracy > 0.8:
                results.append("â€¢ Model performance: Excellent!")
            elif accuracy > 0.6:
                results.append("â€¢ Model performance: Good")
            else:
                results.append("â€¢ Model performance: Needs improvement")

            # Feature importance simulation
            results.append("â€¢ Feature importance:")
            features_importance = {
                'original': random.uniform(0.3, 0.8),
                'squared': random.uniform(0.1, 0.5),
                'sqrt': random.uniform(0.2, 0.6),
                'log': random.uniform(0.1, 0.4)
            }

            for feature, importance in sorted(features_importance.items(), key=lambda x: x[1], reverse=True):
                results.append(f"  - {feature}: {importance:.3f}")
        else:
            results.append("â€¢ No data available for ML simulation")

    if analysis_type in ["visualization", "all"]:
        results.append("ðŸ“Š **Data Visualization**")
        results.append("-" * 30)

        if data:
            results.append("â€¢ Visualization data prepared:")
            results.append(f"  - Dataset size: {len(data)} points")
            results.append(f"  - Data range: {min(data):.2f} to {max(data):.2f}")
            results.append(f"  - Data type: {'Numeric' if all(isinstance(x, (int, float)) for x in data) else 'Mixed'}")

            # Simple histogram simulation
            if len(data) > 5:
                min_val, max_val = min(data), max(data)
                bin_width = (max_val - min_val) / 5
                histogram = [0] * 5

                for val in data:
                    bin_index = min(4, int((val - min_val) / bin_width))
                    histogram[bin_index] += 1

                results.append("â€¢ Histogram (5 bins):")
                for i, count in enumerate(histogram):
                    bin_start = min_val + i * bin_width
                    bin_end = min_val + (i + 1) * bin_width
                    bar = "â–ˆ" * int(count * 20 / max(histogram))
                    results.append(f"  {bin_start:.1f}-{bin_end:.1f}: {bar} ({count})")

            # Trend analysis
            if len(data) > 3:
                trend = "increasing" if data[-1] > data[0] else "decreasing" if data[-1] < data[0] else "stable"
                results.append(f"â€¢ Trend analysis: {trend}")
        else:
            results.append("â€¢ No data available for visualization")

    # Performance metrics
    results.append("âš¡ **Analysis Performance**")
    results.append("-" * 30)
    results.append(f"â€¢ Analysis completed at: {datetime.now().strftime('%H:%M:%S')}")
    results.append(f"â€¢ Data processing time: {random.uniform(0.001, 0.01):.3f} seconds")
    results.append(f"â€¢ Memory usage: {random.randint(1000000, 5000000)} bytes")

    results.append("")
    results.append("ðŸŽ‰ **Data Analysis Complete!**")
    results.append("ðŸ’¡ This demonstrates Python's data analysis capabilities with shadcn/ui!")

    return "\n".join(results)

# Create the data analysis demo interface
demo = cl.Interface(
    fn=data_analysis_demo,
    inputs=[
        Input(
            placeholder="basic, distribution, correlation, machine_learning, visualization, all",
            label="Analysis Type",
            value="basic"
        ),
        Input(
            placeholder="Enter numbers separated by spaces, or leave empty for generated data",
            label="Data Input",
            value=""
        ),
        Input(
            placeholder="Additional parameters (optional)",
            label="Parameters",
            value=""
        )
    ],
    outputs="text",
    title="ðŸ“Š Python Data Analysis & ML Demo",
    description="Explore Python's data science ecosystem with beautiful shadcn/ui interfaces! ðŸš€"
)

if __name__ == "__main__":
    print("ðŸš€ Starting ChaiLab Data Analysis Demo...")
    print("ðŸ“Š This demo showcases Python's data capabilities:")
    print("  â€¢ Statistical analysis and distributions")
    print("  â€¢ Data visualization and plotting")
    print("  â€¢ Machine learning model simulation")
    print("  â€¢ Correlation analysis")
    print("  â€¢ Feature engineering")
    print("  â€¢ Performance benchmarking")
    print("  â€¢ All with beautiful shadcn/ui interface!")
    print()
    print("ðŸ’¡ Try these examples:")
    print("  â€¢ Analysis: 'all' - Complete analysis")
    print("  â€¢ Analysis: 'basic' - Statistics only")
    print("  â€¢ Analysis: 'machine_learning' - ML simulation")
    print("  â€¢ Analysis: 'visualization' - Data visualization")
    print("  â€¢ Leave data input empty to use generated data")
    print()
    print("ðŸŒ Open http://127.0.0.1:7860 in your browser")
    demo.launch()
